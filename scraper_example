import os
import re
import time
import json
from dataclasses import dataclass
from typing import Iterable, Optional, List, Dict, Any

import pandas as pd
from tqdm import tqdm
import praw


ECT_KEYWORDS = [
    r"\bECT\b",
    r"electroconvulsive",
    r"electro[- ]?convulsive",
    r"electroshock",           # include common term people use
    r"shock therapy",
]


@dataclass
class Config:
    client_id: str
    client_secret: str
    user_agent: str

    # Search settings
    subreddits: List[str] = None          # e.g. ["depression", "bipolar", "psychiatry"]
    query: str = "ECT OR electroconvulsive OR electroshock OR \"shock therapy\""
    time_filter: str = "all"              # hour, day, week, month, year, all
    limit_submissions: int = 200          # per subreddit; if subreddits=None uses r/all search once
    max_comments_per_submission: Optional[int] = None  # None = all

    # Output
    out_csv: str = "ect_comments.csv"
    out_jsonl: Optional[str] = "ect_comments.jsonl"

    # Safety / rate-limit pacing
    sleep_seconds: float = 0.0            # set to e.g. 0.2 if you want gentle pacing


def build_reddit(cfg: Config) -> praw.Reddit:
    return praw.Reddit(
        client_id=cfg.client_id,
        client_secret=cfg.client_secret,
        user_agent=cfg.user_agent,
    )


def text_mentions_ect(text: str) -> bool:
    if not text:
        return False
    for pat in ECT_KEYWORDS:
        if re.search(pat, text, flags=re.IGNORECASE):
            return True
    return False


def iter_target_submissions(reddit: praw.Reddit, cfg: Config) -> Iterable[praw.models.Submission]:
    """
    Searches submissions that match cfg.query in chosen subreddits (or r/all).
    """
    if cfg.subreddits:
        for sr in cfg.subreddits:
            subreddit = reddit.subreddit(sr)
            for sub in subreddit.search(cfg.query, time_filter=cfg.time_filter, limit=cfg.limit_submissions):
                yield sub
    else:
        # one pass over r/all
        subreddit = reddit.subreddit("all")
        for sub in subreddit.search(cfg.query, time_filter=cfg.time_filter, limit=cfg.limit_submissions):
            yield sub


def extract_comments_from_submission(
    submission: praw.models.Submission,
    cfg: Config,
) -> List[Dict[str, Any]]:
    """
    Pulls ALL comments from a submission (submission.comments.replace_more()).
    Filters to comments that mention ECT (configurable).
    """
    # Expand all comments (can be slow on huge threads)
    submission.comments.replace_more(limit=None)

    rows: List[Dict[str, Any]] = []
    count = 0

    for c in submission.comments.list():
        if cfg.max_comments_per_submission is not None and count >= cfg.max_comments_per_submission:
            break

        body = getattr(c, "body", "") or ""
        if not text_mentions_ect(body):
            continue

        author = None
        try:
            author = c.author.name if c.author else None
        except Exception:
            author = None

        rows.append(
            {
                "submission_id": submission.id,
                "submission_title": submission.title,
                "submission_permalink": f"https://www.reddit.com{submission.permalink}",
                "submission_created_utc": int(submission.created_utc),
                "subreddit": str(submission.subreddit),
                "comment_id": c.id,
                "comment_parent_id": c.parent_id,
                "comment_created_utc": int(getattr(c, "created_utc", 0) or 0),
                "comment_score": int(getattr(c, "score", 0) or 0),
                "comment_author": author,
                "comment_body": body,
            }
        )
        count += 1

    return rows


def run(cfg: Config) -> pd.DataFrame:
    reddit = build_reddit(cfg)

    seen_submission_ids = set()
    all_rows: List[Dict[str, Any]] = []

    submissions = list(iter_target_submissions(reddit, cfg))

    for sub in tqdm(submissions, desc="Harvesting submissions"):
        if sub.id in seen_submission_ids:
            continue
        seen_submission_ids.add(sub.id)

        try:
            rows = extract_comments_from_submission(sub, cfg)
            all_rows.extend(rows)
        except Exception as e:
            # Skip problematic threads but keep going
            print(f"[WARN] Failed on submission {sub.id}: {e}")

        if cfg.sleep_seconds:
            time.sleep(cfg.sleep_seconds)

    df = pd.DataFrame(all_rows)

    # De-dup by comment_id just in case
    if not df.empty and "comment_id" in df.columns:
        df = df.drop_duplicates(subset=["comment_id"]).reset_index(drop=True)

    # Write outputs
    df.to_csv(cfg.out_csv, index=False)

    if cfg.out_jsonl:
        with open(cfg.out_jsonl, "w", encoding="utf-8") as f:
            for row in all_rows:
                f.write(json.dumps(row, ensure_ascii=False) + "\n")

    return df


if __name__ == "__main__":
    # Prefer env vars so you don't hardcode secrets
    client_id = os.getenv("REDDIT_CLIENT_ID", "").strip()
    client_secret = os.getenv("REDDIT_CLIENT_SECRET", "").strip()
    user_agent = os.getenv("REDDIT_USER_AGENT", "ect-comments-scraper/1.0 by yourusername").strip()

    if not (client_id and client_secret and user_agent):
        raise SystemExit(
            "Missing Reddit credentials. Set env vars:\n"
            "  REDDIT_CLIENT_ID\n"
            "  REDDIT_CLIENT_SECRET\n"
            "  REDDIT_USER_AGENT\n"
        )

    cfg = Config(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent,
        # Optional: target specific communities for better signal
        subreddits=[
            "depression",
            "bipolar",
            "psychotherapy",
            "mentalhealth",
            "psychiatry",
            "AskPsychiatry",
            "SuicideWatch",
            "Anxiety",
            "dpdr",
            "schizoaffective",
        ],
        limit_submissions=200,
        time_filter="all",
        out_csv="ect_comments.csv",
        out_jsonl="ect_comments.jsonl",
        sleep_seconds=0.0,
    )

    df = run(cfg)
    print(f"Done. Rows: {len(df)}. Saved: {cfg.out_csv}" + (f" and {cfg.out_jsonl}" if cfg.out_jsonl else ""))
